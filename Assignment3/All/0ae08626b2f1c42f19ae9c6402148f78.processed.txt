   #[1]RSS 2.0 [2]RSS .92 [3]Atom 1.0

[4]Picking N distinct numbers at random: how to do it fast?

   To test my algorithms, I like to generate synthetic data. To do so, I
   often need to generate distinct randomly chosen numbers from a range of
   values. For example, maybe I want to pick 2 distinct integers in the
   interval [0,10]. For my purposes, I need these numbers to appear in
   order, but we can just generate them in any order and sort them later.

   Picking the first number at random is easy: most programming languages
   come with fast pseudo-random number generators. However, when you try
   to pick the second number, there is a small probability that you pick
   the first one again. If this happens, you need to start again. To check
   quickly whether a number has already been picked, we might use a hash
   table. This suggests the first algorithm one might try:

HashSet<Integer> s = new HashSet<Integer>();
while (s.size() < N)
  s.add(rand.nextInt(Max));

   (This code generates N distinct integers in the interval [0,Max).)

   Intuitively, this algorithm is hard to beat when you need to pick few
   integers from a large range. In this case, the probability that you
   will pick an already picked number is small. But, in fact, even if you
   need to pick one out of every two values from a range (say pick 10
   integers in the interval [0, 20)), this algorithm is still reasonably
   efficient. Indeed, the probability that a given number is already
   picked is no larger than 50%. How many times (on average) do you need
   to generate a new random numbers if you have a 50% probability of
   rejecting them? You can check that the answer 2. This means that as
   long as you don't need to pick more than half the values (N is no more
   than Max/2), you can expect to need to generate no more than Max random
   numbers.

   What if you need to pick more than Max/2 integers in [0,Max)? This can
   become a problem if you are not careful. Thankfully, there is a nice
   fix: picking N integers in [0,Max) for N large to Max is equivalent to
   picking Max-N integers in [0,Max) and then selecting the numbers you
   did not pick. Computing this complement can be done efficiently if you
   first sort the numbers you picked. This means that you can always
   assume that N is no larger than Max/2.

   Still, it is reasonable to think that the performance of the hash-based
   algorithm degrade as N becomes closer to Max/2.

   One possibly better alternative in this case... one that your typical
   Computer Science professor might propose... is [5]Reservoir Sampling.
   Though it sounds fancy, Reservoir Sampling is actually easily
   implemented:

        int[] ans = new int[N];
        for (int k = 0; k < N; ++k)
                ans[k]=k;
        for(int k = N ; k < Max; ++k) {
                int v = rand.nextInt(k+1);
                if(v < N) {
                        ans[v] = k;
                }
        }

   It is not immediately obviously why this algorithm would work. However,
   it is correct. The nice thing about Reservoir Sampling is that we know
   exactly how many random numbers we need to generate: we need Max of
   them, no matter what. This means that Reservoir Sampling has a running
   time that depends on Max, but not a lot of N.

   However, it turns out that an even better alternative might be to
   replace the hash table by a bitmap. A bitmap is just an array of bits.
   We need Max bits. If the value has already been picked, we set the bit
   to 1, otherwise the bit is set to 0. The algorithm is otherwise
   identical to the first hash-based algorithm:

        BitSet bs = new BitSet(Max);
        int cardinality = 0;
        while(cardinality < N) {
                int v = rand.nextInt(Max);
                if(!bs.get(v)) {
                        bs.set(v);
                        cardinality++;
                }
        }

   It turns out that a good heuristic is to use the bitmap algorithm when
   N is smaller than  Max / 1024. Otherwise the hash-based algorithm
   appears better. Reservoir Sampling is not a good choice for this
   problem.

   The following table shows the speed (in millions of integers picked per
   second) of the various techniques on a recent i7 processor using C++.
   Note how much faster the bitmap approach is.
    Max/N   Hash   Bitmap   Reservoir Sampling
   16384   2.0    1.0      0.0
   1024    7.5    28       0.1
   2       1.3    64       14

   For good measure, I coded up these algorithms in both Java and C++. The
   results are consistent. My code is [6]available for review.

   Credit: I thank to Nathan Kurz for challenging me on this problem.
   [7]Tweet
   [8]Comments (11)

   Related posts (automatically generated):
     * [9]Fast bitmap decoding

11 Comments

    1. I wonder how this compares?
       (1) fill an array with 0 to Max random floating point numbers
       (2) apply an index sort to the array (sort the array and return an
       index of elements in ascending order)
       (3) output the first N values in the index
       Comment by [10]Peter Turney -- 24/8/2013 @ [11]13:53
    2. In Perl Data Language, this algorithm takes two lines:
       $result = qsorti(random($max));
       p $result(0:($n-1));
       Comment by [12]Peter Turney -- 24/8/2013 @ [13]17:52
    3. @Peter Turney
       I don't think your numbers are going to be distinct. In theory, it
       is possible that your approach would pick just one distinct value
       (repeated many times).
       Update: I misread Peter's description.
       Comment by [14]Daniel Lemire -- 26/8/2013 @ [15]6:32
    4. You're wrong. I've been using exactly this code for years.
       Comment by [16]Peter Turney -- 26/8/2013 @ [17]6:37
    5. @Peter Turney
       I misread your algorithm. Sorry.
       Still, there is a probability (very small indeed) that all your
       floating point numbers are going to identical. In this sense, your
       algorithm is probabilistic... with good probability, it will solve
       the problem.
       I'll add it to my benchmark later. Thanks.
       Comment by [18]Daniel Lemire -- 26/8/2013 @ [19]6:49
    6. An example might help:
       pdl> $ran=random(4)
       pdl> p $ran
       [0.9474 0.8675 0.7389 0.4402]
       pdl> $sort=qsorti($ran)
       pdl> p $sort
       [3 2 1 0]
       (p = print; floats are truncated for display purposes; sort from
       smallest to largest; qsorti = quick sort and return index)
       When you're using a high-level language (Perl Data Language,
       Matlab, etc.), you learn to avoid explicit loops by calling
       built-in functions that implicitly loop over vectors and matrices.
       In a high-level language, an algorithm without explicit loops is
       almost always much faster than an algorithm with explicit loops.
       Comment by [20]Peter Turney -- 26/8/2013 @ [21]6:50
    7. "Still, there is a probability (very small indeed) that all your
       floating point numbers are going to identical. In this sense, your
       algorithm is probabilistic... with good probability, it will solve
       the problem."
       If all the floating point numbers are identical and the sort
       preserves the original order when there is a tie, then the output
       will be [0 1 2 3 ...]. It is not a problem if this output happens
       from time to time. It is a valid output, as long as it doesn't
       happen too often.
       Comment by [22]Peter Turney -- 26/8/2013 @ [23]6:53
    8. @Peter Turney
       That's right, but even a tie between two floating point numbers, if
       ties are always resolved in the same deterministic manner, will
       introduce a bias.
       I assume that Perl will use 64-bit floating point numbers... in
       such a case, your algorithm to generate distinct 32-bit integers
       has a negligible bias.
       (I should note that even my algorithms have biases in practice.
       They are only free of biases if I assume that I have a perfect
       random number generators.)
       Of course, the interesting question is speed. The way you describe
       your algorithm, it runs in time O(Max), so that we might expect
       that when N is much smaller than Max, then your algorithm is slow
       compared to the hash set approach. My instinct is that even when N
       is close to Max, your algorithm is slower than the bitmap approach.
       Of course, I'll need to verify this more seriously.
       Comment by [24]Daniel Lemire -- 26/8/2013 @ [25]7:15
    9. "The way you describe your algorithm, it runs in time O(Max)"
       Actually O(Max log(Max)) due to sorting. But if you implement all
       the algorithms in a high-level language, I'm guessing mine will run
       the fastest, due to the lack of explicit loops. For me, the time
       and effort I save by writing programs in a high-level language is
       more important than the speed I can get from the computer by
       working in a lower-level language (in most cases, with some rare
       exceptions).
       Comment by [26]Peter Turney -- 26/8/2013 @ [27]7:37
   10. Also, what if you don't want the final list in sorted order? What
       if the task is to shuffle the numbers in the list [0,Max]? (This is
       a typical task for my work.) Then your N/2 select/drop inversion
       trick is not applicable.
       Comment by [28]Peter Turney -- 26/8/2013 @ [29]7:48
   11. @Peter Turney
       As for focusing on the performance one gets using Perl... see my
       post "The language interpreters are the new machines":
       [30]http://lemire.me/blog/archives/2011/06/14/the-language-interpre
       ters-are-the-new-machines/
       Much of the traditional computer science is focused on designing
       algorithms from basic operations (and this is what I do here with
       C++ and Java), but this is increasingly less relevant.
       But not everything is black and white. For some problems, it is
       definitively worth it to get a 10x speed-up. Google's backend could
       not be written in pure Perl. ;-)
       Back to the problem at hand...
       If you want to shuffle a list, then a Fisher-Yates shuffle is
       probably best. Such shuffling is part of Java, C++(STL) and Python.
       I don't know about Perl but I have read online that you can find a
       shuffle function in List::Util. So I would argue that in many
       instances, you shouldn't code a list shuffling algorithm by hand.
       Comment by [31]Daniel Lemire -- 26/8/2013 @ [32]10:25

   Sorry, the comment form is closed at this time.
   « « [33]Previous: Privacy and the Internet: Is Facebook evil?
   [34]Next: Funding science: When bureaucrats get out of control » »

   [35]« Blog's main page
   Daniel Lemire's picture
     * [36]Daniel Lemire's blog
       Canadian flag Montreal, Canada
     * Google Plus logo [37]Follow on Google Plus
       22,500 followers
     * twitter logo [38]Follow @lemire
       4,000 followers
     * Facebook logo [39]Follow on Facebook
     * Google Scholar logo [40]Follow on Google Scholar
     * Subscribe to this blog
       - [41]in a reader,
       - [42]on your kindle,
       - or ____________________ Subscribe by email
     * Search through 1420 posts and 5847 comments:
       ____________________
       Search
     *
          + [43]About me
          + [44]Book recommendations
          + [45]My readers
          + [46]Terms of use
          + [47]Write good papers
     * Recent Comments:
          + Thierry Lhôte on [48]The written word took over the world
          + Thierry Lhôte on [49]The written word took over the world
          + Ben on [50]The written word took over the world
          + Anonymous on [51]The written word took over the world
          + kilian on [52]The written word took over the world
     * Some popular posts
          + [53]Why I still program
          + [54]Emotions killing your intellectual productivity
          + [55]Turn your weaknesses into strengths
          + [56]It is not where you work, but who you work with
     * [57]Home page
       [58]Google Scholar profile
       [59]arXiv
       [60]DBLP

   Powered by [61]WordPress

   © 2004-2013, [62]Daniel Lemire (lemire at gmail dot com). This work is
   licensed under a [63]Creative Commons License.

References

   Visible links
   1. http://lemire.me/blog/feed/
   2. http://lemire.me/blog/feed/rss/
   3. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/atom
   4. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/
   5. http://en.wikipedia.org/wiki/Reservoir_sampling
   6. https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/tree/master/2013/08/14
   7. https://twitter.com/share
   8. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/#comments
   9. http://lemire.me/blog/archives/2012/05/21/fast-bitmap-decoding/
  10. http://www.apperceptual.com/
  11. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91758
  12. http://www.apperceptual.com/
  13. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91761
  14. http://lemire.me/en/
  15. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91799
  16. http://www.apperceptual.com/
  17. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91800
  18. http://lemire.me/en/
  19. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91801
  20. http://www.apperceptual.com/
  21. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91802
  22. http://www.apperceptual.com/
  23. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91804
  24. http://lemire.me/en/
  25. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91805
  26. http://www.apperceptual.com/
  27. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91806
  28. http://www.apperceptual.com/
  29. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91807
  30. http://lemire.me/blog/archives/2011/06/14/the-language-interpreters-are-the-new-machines/
  31. http://lemire.me/en/
  32. http://lemire.me/blog/archives/2013/08/16/picking-n-distinct-numbers-at-random-how-to-do-it-fast/?utm_source=feedburner#comment-91812
  33. http://lemire.me/blog/archives/2013/08/09/is-facebook-evil/
  34. http://lemire.me/blog/archives/2013/08/28/funding-science-when-bureaucrats-get-out-of-control/
  35. http://lemire.me/blog/
  36. http://lemire.me/
  37. https://plus.google.com/105888615414982242080/about
  38. http://www.twitter.com/lemire/
  39. http://www.facebook.com/daniel.lemire
  40. http://scholar.google.com/citations?sortby=pubdate&hl=en&user=q1ja-G8AAAAJ&view_op=list_works
  41. http://lemire.me/blog/feed/
  42. http://www.amazon.com/Daniel-Lemires-blog/dp/B002DPV7QQ?SubscriptionId=AKIAILSHYYTFIVPWUY6Q
  43. http://lemire.me/blog/about-me/
  44. http://lemire.me/blog/book-recommendations/
  45. http://lemire.me/blog/my-readers/
  46. http://lemire.me/blog/terms-of-use/
  47. http://lemire.me/blog/rules-to-write-a-good-research-paper/
  48. http://lemire.me/blog/archives/2013/10/01/the-written-word/#comment-95660
  49. http://lemire.me/blog/archives/2013/10/01/the-written-word/#comment-95652
  50. http://lemire.me/blog/archives/2013/10/01/the-written-word/#comment-95627
  51. http://lemire.me/blog/archives/2013/10/01/the-written-word/#comment-95596
  52. http://lemire.me/blog/archives/2013/10/01/the-written-word/#comment-95584
  53. http://lemire.me/blog/archives/2011/06/06/why-i-still-program/
  54. http://lemire.me/blog/archives/2009/01/20/emotions-killing-your-intellectual-productivity/
  55. http://lemire.me/blog/archives/2009/01/19/turn-your-weaknesses-into-strengths/
  56. http://lemire.me/blog/archives/2011/10/25/it-is-not-where-you-work-but-who-you-work-with/
  57. http://lemire.me/en/
  58. http://scholar.google.com/citations?sortby=pubdate&hl=en&user=q1ja-G8AAAAJ&view_op=list_works
  59. http://arxiv.org/a/lemire_d_1
  60. http://www.informatik.uni-trier.de/~ley/db/indices/n-tree/l/Lemire:Daniel.html
  61. http://wordpress.org/
  62. http://lemire.me/en/
  63. http://creativecommons.org/licenses/by-sa/2.0/

   Hidden links:
  64. http://lemire.me/blog/feed/
